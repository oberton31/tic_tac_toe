{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKU2iY_7at8Y"
      },
      "outputs": [],
      "source": [
        "!pip install tf-agents[reverb]\n",
        "!pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPuD0bMEY9Iz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZAoFNwnRbKK"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HD0cDykPL6I"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "'''\n",
        "0 1 2\n",
        "3 4 5\n",
        "6 7 8\n",
        "'''\n",
        "class TicTacToeEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=8, name='action') # user can choose to put on one of 9 places on board\n",
        "\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(1,9), dtype=np.int32, minimum=0, maximum=2, name='observation') # 3 by 3 grid\n",
        "\n",
        "        self._episode_ended = False\n",
        "        self._state = [0,0,0,0,0,0,0,0,0]\n",
        "        self.current_player = 1 # human player\n",
        "        self._outcome = 0 # 0 for no winner, 1 for player 1 wins, 2 for player 2, and 3 for draw\n",
        "        self._moves = 0\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self._episode_ended = False\n",
        "        self.current_player = 1\n",
        "        self._state = [0,0,0,0,0,0,0,0,0]\n",
        "        self._moves = 0\n",
        "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "\n",
        "\n",
        "    def _check_game_over(self):\n",
        "        for j in range(3):\n",
        "          if self._state[j*3] == self._state[j*3 + 1] == self._state[j*3 + 2] != 0:\n",
        "            self._outcome = self._state[j*3]  # Corrected assignment\n",
        "            return True  # Row win\n",
        "\n",
        "        for i in range(3):\n",
        "          if self._state[i] == self._state[3+i] == self._state[6+i] != 0:\n",
        "            self._outcome = self._state[i]  # Corrected assignment\n",
        "            return True  # Column win\n",
        "\n",
        "        if self._state[0] == self._state[4] == self._state[8] != 0:\n",
        "            self._outcome = self._state[0]  # Corrected assignment\n",
        "            return True  # Diagonal win\n",
        "        if self._state[2] == self._state[4] == self._state[6] != 0:\n",
        "            self._outcome = self._state[2]  # Corrected assignment\n",
        "            return True  # Anti-diagonal win\n",
        "\n",
        "        if 0 in self._state:\n",
        "            return False  # Game is still ongoing\n",
        "\n",
        "        self._outcome = 3\n",
        "        return True  # Draw (no winner)\n",
        "\n",
        "\n",
        "    def _change_board(self, action, player):\n",
        "        if action < 3:\n",
        "            self._state[action][0] = player\n",
        "        elif action < 6:\n",
        "            self._state[action-3][1] = player\n",
        "        elif action < 9:\n",
        "            self._state[action-6][2] = player\n",
        "\n",
        "    def _check_block_opponent(self):\n",
        "        opponent = 3 - self.current_player  # Determine the opponent's mark\n",
        "        # Iterate through all possible winning combinations\n",
        "        for combination in [[0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]:\n",
        "            # Check if the opponent has two marks in a winning combination and the third position is empty\n",
        "            if (self._state[combination[0]] == opponent and\n",
        "                self._state[combination[1]] == opponent and\n",
        "                self._state[combination[2]] == 0):\n",
        "                return True  # Blocking move detected\n",
        "            elif (self._state[combination[0]] == opponent and\n",
        "                  self._state[combination[2]] == opponent and\n",
        "                  self._state[combination[1]] == 0):\n",
        "                return True  # Blocking move detected\n",
        "            elif (self._state[combination[1]] == opponent and\n",
        "                  self._state[combination[2]] == opponent and\n",
        "                  self._state[combination[0]] == 0):\n",
        "                return True  # Blocking move detected\n",
        "        return False  # No blocking move detected\n",
        "\n",
        "\n",
        "    def _step(self, action):\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        reward = 0\n",
        "        if self._state[action] == 0:\n",
        "            self._state[action] = self.current_player\n",
        "        else:\n",
        "            reward = -1  # Invalid move, penalize the agent\n",
        "\n",
        "        # Check if the agent's move blocks the opponent from winning\n",
        "        if self._check_block_opponent():\n",
        "            reward += 1  # Add a positive reward for blocking the opponent's winning move\n",
        "\n",
        "        self._episode_ended = self._check_game_over()\n",
        "\n",
        "        self._moves += 1\n",
        "        if self._moves == 9:\n",
        "            self._episode_ended = True\n",
        "            if not self._outcome:\n",
        "                reward = 0  # Draw\n",
        "        self.current_player = 3 - self.current_player  # Toggle between 1 and 2\n",
        "\n",
        "        if self._episode_ended:\n",
        "            if self._outcome == 3:\n",
        "                reward = 0\n",
        "            elif self._outcome == 1:\n",
        "                reward = 5\n",
        "            elif self._outcome == 2:\n",
        "                reward = -5\n",
        "\n",
        "            return ts.termination(\n",
        "                np.array([self._state], dtype=np.int32), reward)\n",
        "        else:\n",
        "            return ts.transition(\n",
        "                np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)\n",
        "\n",
        "\n",
        "    # Function to convert state to index\n",
        "    def state_to_index(self, state):\n",
        "        return int(''.join(map(str, state)), 3)\n",
        "\n",
        "    def choose_action(self, state, epsilon, agent):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.randint(9)\n",
        "        elif agent == 1:\n",
        "            return np.argmax(Q_table_1[self.state_to_index(state)])\n",
        "        else:\n",
        "            return np.argmax(Q_table_1[self.state_to_index(state)])\n",
        "\n",
        "    def opponent_strategy(self, state):\n",
        "        # Check if opponent can win in the next move\n",
        "        for i in range(9):\n",
        "            if state[i] == 0:\n",
        "                state[i] = 2\n",
        "                if self._check_game_over():\n",
        "                    state[i] = 0\n",
        "                    return i\n",
        "                state[i] = 0\n",
        "\n",
        "        # Check if player can win in the next move and block it\n",
        "        for i in range(9):\n",
        "            if state[i] == 0:\n",
        "                state[i] = 1\n",
        "                if self._check_game_over():\n",
        "                    state[i] = 0\n",
        "                    return i\n",
        "                state[i] = 0\n",
        "\n",
        "        # Choose a random empty position\n",
        "        empty_positions = [i for i in range(9) if state[i] == 0]\n",
        "        if empty_positions:\n",
        "          return np.random.choice(empty_positions)\n",
        "        else:\n",
        "          return 0\n",
        "\n",
        "    def train(self, num_episodes, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        Q_table_1 = np.zeros((3**9, 9))  # Q-table size: number of possible states x number of actions\n",
        "\n",
        "        # Function to convert state to index\n",
        "        def state_to_index(state):\n",
        "            return int(''.join(map(str, state)), 3)\n",
        "\n",
        "        # Function to choose action based on epsilon-greedy policy\n",
        "        def choose_action(state, epsilon):\n",
        "            if np.random.rand() < epsilon:\n",
        "                return np.random.randint(9)\n",
        "            else:\n",
        "                return np.argmax(Q_table_1[state_to_index(state)])\n",
        "\n",
        "        # Training loop\n",
        "        for episode in range(num_episodes):\n",
        "            time_step = self.reset()\n",
        "            state = time_step.observation[0]\n",
        "            done = False\n",
        "\n",
        "            total_reward = 0\n",
        "\n",
        "            while not done:\n",
        "                # Agent 1's turn\n",
        "                action = choose_action(state, epsilon)\n",
        "                time_step = self.step(action)\n",
        "                next_state = time_step.observation[0]\n",
        "                reward = time_step.reward\n",
        "                done = time_step.is_last()\n",
        "\n",
        "                # Update Q-value for Agent 1\n",
        "                next_state_index = state_to_index(next_state)\n",
        "                if not done:\n",
        "                    Q_table_1[state_to_index(state), action] += alpha * (reward + gamma * np.max(Q_table_1[next_state_index]) - Q_table_1[state_to_index(state), action])\n",
        "                else:\n",
        "                    Q_table_1[state_to_index(state), action] += alpha * (reward - Q_table_1[state_to_index(state), action])\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                # Player 2's turn (random move)\n",
        "                action = self.opponent_strategy(state)  # Randomly choose an action\n",
        "                time_step = self.step(action)\n",
        "                next_state = time_step.observation[0]\n",
        "                reward = time_step.reward\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            # Print progress\n",
        "            if episode % 100 == 0:\n",
        "                print(\"Episode {}: Total Reward for Agent 1 = {}\".format(episode, total_reward))\n",
        "\n",
        "        return Q_table_1\n",
        "\n",
        "\n",
        "    def play_against_agent(self, Q_table, first_turn):\n",
        "        time_step = self.reset()\n",
        "        current_state = time_step.observation[0]\n",
        "        done = False\n",
        "\n",
        "        if first_turn == 2:\n",
        "          action = np.argmax(Q_table[self.state_to_index(current_state)])\n",
        "        while not done:\n",
        "            print(\"Current Board:\")\n",
        "            self.print_board(current_state)\n",
        "\n",
        "            if self.current_player == 1:\n",
        "                action = int(input(\"Your move (0-8): \"))\n",
        "                if action < 0 or action > 8:\n",
        "                    print(\"Invalid move. Please choose a number between 0 and 8.\")\n",
        "                    continue\n",
        "            else:\n",
        "                action = np.argmax(Q_table[self.state_to_index(current_state)])\n",
        "\n",
        "            time_step = self.step(action)\n",
        "            current_state = time_step.observation[0]\n",
        "            done = time_step.is_last()\n",
        "\n",
        "        print(\"Game Over!\")\n",
        "        print(\"Result: \", end=\"\")\n",
        "        if self._outcome == 1:\n",
        "            print(\"You win!\")\n",
        "        elif self._outcome == 2:\n",
        "            print(\"Agent 1 wins!\")\n",
        "        else:\n",
        "            print(\"It's a draw!\")\n",
        "\n",
        "    def print_board(self, state):\n",
        "        symbols = ['_', 'X', 'O']\n",
        "        for i in range(3):\n",
        "            print(\" \".join(symbols[state[j + i*3]] for j in range(3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hhm-5R7spVx"
      },
      "outputs": [],
      "source": [
        "environment = TicTacToeEnv()\n",
        "utils.validate_py_environment(environment, episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = TicTacToeEnv()\n",
        "Q_table_1 = env.train(num_episodes=5000)"
      ],
      "metadata": {
        "id": "jUX6wVaZmRxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_q_table(Q_table, filename):\n",
        "    np.save(filename, Q_table)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming Q_table is the trained Q-table and filename is the path to save the file\n",
        "save_q_table(Q_table_1, 'q_table.npy')"
      ],
      "metadata": {
        "id": "fWpSAa2idWUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env.play_against_agent(Q_table_1, 2)"
      ],
      "metadata": {
        "id": "UIUWE2DentBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}